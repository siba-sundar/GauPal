{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emVZG-5CsqTJ",
        "outputId": "4195064c-3e7b-4489-cf21-9b4e8a4d2b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Install Libraries\n",
        "!pip install deap openpyxl scikit-learn --quiet\n",
        "print(\"Required libraries checked/installed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsT5xd7ftReF",
        "outputId": "898efdbe-9ce7-4ea0-d93a-a290571b47c5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required libraries checked/installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost --quiet\n",
        "print(\"XGBoost installation checked.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzbuhFpPAJA9",
        "outputId": "b0174f5f-4e71-4e11-a6b0-a4a85f39f3cb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost installation checked.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Import Libraries & Mount Google Drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import os\n",
        "import time\n",
        "import traceback # For detailed error printing\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Import DEAP components after installation\n",
        "try:\n",
        "    from deap import base, creator, tools, algorithms\n",
        "except ImportError:\n",
        "    print(\"DEAP library not found after installation attempt. Please check installation.\")\n",
        "    raise SystemExit(\"DEAP required.\")\n",
        "\n",
        "\n",
        "print(\"Libraries imported.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCrDADQ3tTQ_",
        "outputId": "f4170c62-3a36-43a3-9540-1a97afb34f41"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Define Data Path, Target Column, Threshold & Load Data\n",
        "\n",
        "# === Configuration ===\n",
        "# --- PATH TO YOUR *NEW* DATASET (with correct CCS score) ---\n",
        "data_path = '/content/drive/MyDrive/MatchFound.xlsx' # <--- *** REPLACE THIS PATH ***\n",
        "# --- NAME OF THE COLUMN WITH THE RAW CCS SCORE ---\n",
        "target_score_column = 'Compatibility_Score' # <--- *** VERIFY/CHANGE THIS COLUMN NAME ***\n",
        "# --- THRESHOLD FOR YES/NO CLASSIFICATION ---\n",
        "compatibility_threshold = 20 # <--- *** VERIFY/CHANGE THIS THRESHOLD ***\n",
        "# --- PATH TO SAVE FINAL MODEL COMPONENTS ---\n",
        "# Create the directory for saving if it doesn't exist\n",
        "model_save_directory = '/content/drive/MyDrive/MyModels' # Define directory\n",
        "model_save_path = os.path.join(model_save_directory, 'cattle_predictor_v5.pkl') # Define full path\n",
        "if not os.path.exists(model_save_directory):\n",
        "    print(f\"Creating save directory: {model_save_directory}\")\n",
        "    os.makedirs(model_save_directory)\n",
        "\n",
        "\n",
        "print(f\"Attempting to load data from: {data_path}\")\n",
        "if not os.path.exists(data_path):\n",
        "    print(f\"ERROR: File not found: {data_path}\"); raise SystemExit(\"Dataset not found.\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_excel(data_path, engine='openpyxl')\n",
        "    print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
        "    if target_score_column not in df.columns:\n",
        "        print(f\"ERROR: Target score column '{target_score_column}' not found!\")\n",
        "        print(f\"Available columns: {df.columns.tolist()}\")\n",
        "        raise SystemExit(\"Target column missing.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading XLSX: {e}\"); raise SystemExit(\"Data loading failed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkiBQgLNtUtL",
        "outputId": "b545d4c4-e85f-49a4-80f8-27d4e45c99bf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load data from: /content/drive/MyDrive/MatchFound.xlsx\n",
            "Dataset loaded successfully. Shape: (8000, 38)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Create Targets, Engineer Features & Define Final X/y\n",
        "\n",
        "# Create Binary Classification Target\n",
        "binary_target_column = 'Compatible_Class'\n",
        "df[binary_target_column] = (df[target_score_column] >= compatibility_threshold).astype(int)\n",
        "\n",
        "print(f\"\\nCreated binary target '{binary_target_column}' based on threshold >= {compatibility_threshold}\")\n",
        "print(df[binary_target_column].value_counts(normalize=True))\n",
        "\n",
        "# Define Targets (y)\n",
        "y_reg = df[target_score_column]\n",
        "y_class = df[binary_target_column]\n",
        "\n",
        "# Define Initial Features (X) - excluding targets and IDs\n",
        "columns_to_exclude = [target_score_column, binary_target_column, 'Cow_ID', 'Bull_ID', 'Compatibility_Score', 'Compatible']\n",
        "columns_to_exclude = [col for col in columns_to_exclude if col in df.columns]\n",
        "X = df.drop(columns=columns_to_exclude)\n",
        "\n",
        "print(f\"\\nInitial Features (X shape): {X.shape}\")\n",
        "\n",
        "# --- FEATURE ENGINEERING ---\n",
        "print(\"\\n--- Engineering New Features ---\")\n",
        "\n",
        "# Ensure required columns exist before creating new features\n",
        "required_cols_for_eng = [\n",
        "    'Cow_Age', 'Bull_Age', 'Cow_Weight', 'Bull_Weight', 'Cow_Height', 'Bull_Height',\n",
        "    'Cow_Milk_Yield', 'Bull_Mother_Milk_Yield', 'Cow_Drought_Resistance',\n",
        "    'Bull_Drought_Resistance', 'Cow_Health_Status', 'Bull_Health_Status',\n",
        "    'Cow_Temperament', 'Bull_Temperament'\n",
        "    # Add other columns if needed by your specific calculations below\n",
        "]\n",
        "\n",
        "missing_req_cols = [col for col in required_cols_for_eng if col not in X.columns]\n",
        "if missing_req_cols:\n",
        "    print(f\"Warning: Missing required columns for feature engineering: {missing_req_cols}. Skipping related features.\")\n",
        "else:\n",
        "    try:\n",
        "        # Example 1: Age Difference\n",
        "        X['FE_Age_Diff'] = abs(X['Cow_Age'] - X['Bull_Age'])\n",
        "\n",
        "        # Example 2: Weight Difference % (handle division by zero)\n",
        "        X['FE_Weight_Diff_Pct'] = np.where(\n",
        "            X['Cow_Weight'] > 0,\n",
        "            abs(X['Cow_Weight'] - X['Bull_Weight']) / X['Cow_Weight'] * 100,\n",
        "            0 # Assign 0 if Cow_Weight is 0 or less\n",
        "        )\n",
        "\n",
        "        # Example 3: Height Difference % (handle division by zero)\n",
        "        X['FE_Height_Diff_Pct'] = np.where(\n",
        "            X['Cow_Height'] > 0,\n",
        "            abs(X['Cow_Height'] - X['Bull_Height']) / X['Cow_Height'] * 100,\n",
        "            0\n",
        "        )\n",
        "\n",
        "        # Example 4: Milk Yield Sum (handle potential NaNs if Milk Yield wasn't imputed yet - though it should be later)\n",
        "        X['FE_Milk_Sum'] = X['Cow_Milk_Yield'].fillna(0) + X['Bull_Mother_Milk_Yield'].fillna(0)\n",
        "\n",
        "        # Example 5: Drought Resistance Difference\n",
        "        X['FE_Drought_Diff'] = abs(X['Cow_Drought_Resistance'] - X['Bull_Drought_Resistance'])\n",
        "\n",
        "        # Example 6: Combined Health Status (simple sum)\n",
        "        X['FE_Combined_Health'] = X['Cow_Health_Status'] + X['Bull_Health_Status']\n",
        "\n",
        "        # Example 7: Temperament Interaction (Simple numeric encoding)\n",
        "        def encode_temperament(row):\n",
        "            cow_t = row['Cow_Temperament']\n",
        "            bull_t = row['Bull_Temperament']\n",
        "            if cow_t == 'Calm' and bull_t == 'Calm': return 0\n",
        "            if (cow_t == 'Calm' and bull_t == 'Aggressive') or \\\n",
        "               (cow_t == 'Aggressive' and bull_t == 'Calm'): return 1\n",
        "            if cow_t == 'Aggressive' and bull_t == 'Aggressive': return 2\n",
        "            return 3 # For unknown/missing combinations\n",
        "\n",
        "        X['FE_Temperament_Combo'] = X.apply(encode_temperament, axis=1)\n",
        "\n",
        "        print(\"Added engineered features based on CCS logic.\")\n",
        "        print(\"New engineered features:\", [col for col in X.columns if col.startswith('FE_')])\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"Error during feature engineering (missing column?): {e}. Skipping feature engineering.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during feature engineering: {e}\")\n",
        "\n",
        "\n",
        "# --- Final Feature Definition ---\n",
        "# X now includes the original features plus the engineered ones\n",
        "print(f\"\\nFinal Features with Engineering (X shape): {X.shape}\")\n",
        "print(f\"Regression Target (y_reg shape): {y_reg.shape}\")\n",
        "print(f\"Classification Target (y_class shape): {y_class.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taU4IJ-StWLf",
        "outputId": "5e2abe49-39ff-476a-ca8b-c8d444d1b667"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Created binary target 'Compatible_Class' based on threshold >= 20\n",
            "Compatible_Class\n",
            "1    0.76675\n",
            "0    0.23325\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Initial Features (X shape): (8000, 34)\n",
            "\n",
            "--- Engineering New Features ---\n",
            "Added engineered features based on CCS logic.\n",
            "New engineered features: ['FE_Age_Diff', 'FE_Weight_Diff_Pct', 'FE_Height_Diff_Pct', 'FE_Milk_Sum', 'FE_Drought_Diff', 'FE_Combined_Health', 'FE_Temperament_Combo']\n",
            "\n",
            "Final Features with Engineering (X shape): (8000, 41)\n",
            "Regression Target (y_reg shape): (8000,)\n",
            "Classification Target (y_class shape): (8000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Preprocessing Setup & Application\n",
        "\n",
        "# Identify feature types\n",
        "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "print(f\"\\nIdentified {len(numerical_features)} numerical features.\")\n",
        "print(f\"Identified {len(categorical_features)} categorical features.\")\n",
        "\n",
        "# Define preprocessing steps (using pipelines)\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]) # handle_unknown is important\n",
        "\n",
        "# Create the preprocessor object\n",
        "# Ensure remainder='passthrough' only if you intend to keep non-numeric/non-categorical columns unprocessed\n",
        "# Usually, it's better to handle all columns explicitly. If X only contains num/cat, remainder='drop' is safer.\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)],\n",
        "    remainder='drop') # Drop columns not specified as numerical or categorical\n",
        "\n",
        "\n",
        "# Fit the preprocessor and transform the data *before* splitting\n",
        "print(\"\\nFitting preprocessor and transforming data...\")\n",
        "try:\n",
        "    X_processed = preprocessor.fit_transform(X)\n",
        "    print(\"Preprocessing complete.\")\n",
        "    # Get feature names after OneHotEncoding\n",
        "    try:\n",
        "        feature_names_out = preprocessor.get_feature_names_out()\n",
        "        print(f\"Total features after preprocessing: {len(feature_names_out)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not get feature names from preprocessor: {e}\")\n",
        "        num_processed_features = X_processed.shape[1]\n",
        "        feature_names_out = [f\"feature_{i}\" for i in range(num_processed_features)]\n",
        "        print(f\"Using generic feature names. Total features: {num_processed_features}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during preprocessing fit_transform: {e}\")\n",
        "    traceback.print_exc()\n",
        "    raise SystemExit(\"Preprocessing failed.\")\n",
        "\n",
        "\n",
        "# Split PREPROCESSED data\n",
        "# Stratify ensures similar class distribution in train/test splits\n",
        "X_train_proc, X_test_proc, y_train_reg, y_test_reg, y_train_class, y_test_class = train_test_split(\n",
        "    X_processed, y_reg, y_class, test_size=0.25, random_state=42, stratify=y_class\n",
        ")\n",
        "\n",
        "print(f\"\\nData split after preprocessing:\")\n",
        "print(f\"X_train_proc shape: {X_train_proc.shape}, X_test_proc shape: {X_test_proc.shape}\")\n",
        "print(f\"y_train_class distribution:\\n{y_train_class.value_counts(normalize=True)}\")\n",
        "print(f\"y_test_class distribution:\\n{y_test_class.value_counts(normalize=True)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRQ85tP2tXm1",
        "outputId": "4aa5142c-1d38-46f5-95f1-ad3236c2da51"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Identified 33 numerical features.\n",
            "Identified 8 categorical features.\n",
            "\n",
            "Fitting preprocessor and transforming data...\n",
            "Preprocessing complete.\n",
            "Total features after preprocessing: 152\n",
            "\n",
            "Data split after preprocessing:\n",
            "X_train_proc shape: (6000, 152), X_test_proc shape: (2000, 152)\n",
            "y_train_class distribution:\n",
            "Compatible_Class\n",
            "1    0.766833\n",
            "0    0.233167\n",
            "Name: proportion, dtype: float64\n",
            "y_test_class distribution:\n",
            "Compatible_Class\n",
            "1    0.7665\n",
            "0    0.2335\n",
            "Name: proportion, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Bull_Milk_Yield']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Genetic Algorithm for Feature Selection Setup (Using XGBoost in Fitness)\n",
        "\n",
        "# --- Add XGBoost imports ---\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_val_score # Already imported but good practice\n",
        "\n",
        "# --- GA Parameters ---\n",
        "# (Keep GA parameters like N_FEATURES, POP_SIZE_FS, NGEN_FS etc. the same)\n",
        "N_FEATURES = X_train_proc.shape[1]\n",
        "POP_SIZE_FS = 50\n",
        "NGEN_FS = 20\n",
        "CXPB_FS = 0.6\n",
        "MUTPB_FS = 0.2\n",
        "WEIGHT_CLASSIFICATION = 0.5\n",
        "WEIGHT_REGRESSION = 0.5\n",
        "\n",
        "# --- Fitness Function ---\n",
        "y_train_reg_std = y_train_reg.std()\n",
        "if y_train_reg_std == 0: y_train_reg_std = 1\n",
        "\n",
        "def evaluate_feature_subset(individual, X_data, y_reg_data, y_class_data, y_reg_std_dev):\n",
        "    \"\"\"Fitness function using XGBoost for evaluation.\"\"\"\n",
        "    selected_indices = [i for i, bit in enumerate(individual) if bit == 1]\n",
        "    if not selected_indices: return (0.0,)\n",
        "\n",
        "    X_subset = X_data[:, selected_indices]\n",
        "\n",
        "    # --- Evaluate Classifier (XGBoost) ---\n",
        "    try:\n",
        "        # *** USE XGBClassifier ***\n",
        "        # scale_pos_weight helps with imbalance. Calculate ratio of majority to minority class.\n",
        "        # Handle potential division by zero or case where only one class exists in y_class_data during CV split\n",
        "        count0 = np.sum(y_class_data == 0)\n",
        "        count1 = np.sum(y_class_data == 1)\n",
        "        scale_pos_weight_val = count0 / count1 if count1 > 0 else 1\n",
        "\n",
        "        clf = xgb.XGBClassifier(n_estimators=30, # Fewer estimators for speed\n",
        "                                random_state=42,\n",
        "                                use_label_encoder=False, # Recommended for newer XGBoost versions\n",
        "                                eval_metric='logloss', # Common classification metric\n",
        "                                scale_pos_weight=scale_pos_weight_val, # Handle imbalance\n",
        "                                n_jobs=1, # Can sometimes conflict with CV n_jobs\n",
        "                                max_depth=6) # Limit depth for speed\n",
        "\n",
        "        class_scores = cross_val_score(clf, X_subset, y_class_data, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
        "        avg_class_score = np.mean(class_scores)\n",
        "    except Exception as e:\n",
        "        # print(f\"Classifier CV Error: {e}\") # Optional debug\n",
        "        avg_class_score = 0.0\n",
        "\n",
        "    # --- Evaluate Regressor (XGBoost) ---\n",
        "    try:\n",
        "        # *** USE XGBRegressor ***\n",
        "        reg = xgb.XGBRegressor(n_estimators=30, # Fewer estimators for speed\n",
        "                               random_state=42,\n",
        "                               eval_metric='rmse',\n",
        "                               n_jobs=1,\n",
        "                               max_depth=6) # Limit depth for speed\n",
        "\n",
        "        reg_scores = cross_val_score(reg, X_subset, y_reg_data, cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
        "        avg_rmse = -np.mean(reg_scores)\n",
        "        normalized_rmse = avg_rmse / y_reg_std_dev if y_reg_std_dev > 0 else avg_rmse\n",
        "        reg_fitness_comp = max(0.0, 1.0 - normalized_rmse)\n",
        "    except Exception as e:\n",
        "        # print(f\"Regressor CV Error: {e}\") # Optional debug\n",
        "        reg_fitness_comp = 0.0\n",
        "\n",
        "    # --- Combine Scores ---\n",
        "    combined_fitness = (WEIGHT_CLASSIFICATION * avg_class_score) + \\\n",
        "                       (WEIGHT_REGRESSION * reg_fitness_comp)\n",
        "\n",
        "    return (combined_fitness,)\n",
        "\n",
        "# --- DEAP Setup for Feature Selection (Binary Individuals) ---\n",
        "# (DEAP setup remains exactly the same as before)\n",
        "creator.create(\"FitnessMaxFS\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"IndividualFS\", list, fitness=creator.FitnessMaxFS)\n",
        "toolbox_fs = base.Toolbox()\n",
        "toolbox_fs.register(\"attr_bool\", random.randint, 0, 1)\n",
        "toolbox_fs.register(\"individual\", tools.initRepeat, creator.IndividualFS, toolbox_fs.attr_bool, N_FEATURES)\n",
        "toolbox_fs.register(\"population\", tools.initRepeat, list, toolbox_fs.individual)\n",
        "toolbox_fs.register(\"evaluate\", evaluate_feature_subset,\n",
        "                    X_data=X_train_proc,\n",
        "                    y_reg_data=y_train_reg,\n",
        "                    y_class_data=y_train_class,\n",
        "                    y_reg_std_dev=y_train_reg_std)\n",
        "toolbox_fs.register(\"mate\", tools.cxUniform, indpb=0.5)\n",
        "toolbox_fs.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
        "toolbox_fs.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "print(\"\\nDEAP toolbox for Feature Selection configured (using XGBoost in fitness).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsh5NCDmtZJV",
        "outputId": "12e76990-a0e9-4807-9983-ac96d1ac8165"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEAP toolbox for Feature Selection configured (using XGBoost in fitness).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMaxFS' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
            "/usr/local/lib/python3.11/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'IndividualFS' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 7. Run GA for Feature Selection\n",
        "\n",
        "print(f\"\\nStarting GA Feature Selection: Population={POP_SIZE_FS}, Generations={NGEN_FS}\")\n",
        "start_time_fs = time.time()\n",
        "\n",
        "pop_fs = toolbox_fs.population(n=POP_SIZE_FS)\n",
        "hof_fs = tools.HallOfFame(1) # Keep only the best\n",
        "\n",
        "stats_fs = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "stats_fs.register(\"avg\", np.mean)\n",
        "stats_fs.register(\"std\", np.std)\n",
        "stats_fs.register(\"min\", np.min)\n",
        "stats_fs.register(\"max\", np.max)\n",
        "\n",
        "# Run the GA\n",
        "try:\n",
        "    algorithms.eaSimple(pop_fs, toolbox_fs, cxpb=CXPB_FS, mutpb=MUTPB_FS, ngen=NGEN_FS,\n",
        "                        stats=stats_fs, halloffame=hof_fs, verbose=True)\n",
        "except Exception as e_ga:\n",
        "    print(f\"Error during GA execution: {e_ga}\")\n",
        "    traceback.print_exc()\n",
        "    raise SystemExit(\"GA failed.\")\n",
        "\n",
        "\n",
        "end_time_fs = time.time()\n",
        "print(f\"GA Feature Selection finished in {end_time_fs - start_time_fs:.2f} seconds.\")\n",
        "\n",
        "# --- Extract Best Feature Set ---\n",
        "if len(hof_fs) == 0:\n",
        "     print(\"ERROR: HallOfFame is empty. GA might not have run correctly or found any valid individuals.\")\n",
        "     # Fallback: Use all features if GA fails? Or stop?\n",
        "     # selected_feature_indices = list(range(N_FEATURES)) # Option: Use all\n",
        "     raise SystemExit(\"GA did not produce a best individual.\")\n",
        "else:\n",
        "    best_individual_fs = hof_fs[0]\n",
        "    selected_feature_indices = [i for i, bit in enumerate(best_individual_fs) if bit == 1]\n",
        "    if not selected_feature_indices:\n",
        "        print(\"WARNING: GA selected zero features. Fitness function or GA parameters might need tuning. Using all features as fallback.\")\n",
        "        selected_feature_indices = list(range(N_FEATURES)) # Fallback to all features\n",
        "    else:\n",
        "         selected_feature_names = [feature_names_out[i] for i in selected_feature_indices]\n",
        "         print(f\"\\nGA selected {len(selected_feature_indices)} features out of {N_FEATURES}.\")\n",
        "         # print(\"Selected feature names:\", selected_feature_names) # Optional"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j5MN6-TtdWw",
        "outputId": "91e95eba-4e19-4c8e-e616-228da5ee7a17"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting GA Feature Selection: Population=50, Generations=20\n",
            "gen\tnevals\tavg     \tstd      \tmin     \tmax     \n",
            "0  \t50    \t0.525597\t0.0928546\t0.382078\t0.671878\n",
            "1  \t40    \t0.609488\t0.0599722\t0.414749\t0.680416\n",
            "2  \t34    \t0.649101\t0.0410421\t0.445992\t0.683529\n",
            "3  \t29    \t0.667375\t0.0166839\t0.599959\t0.693891\n",
            "4  \t39    \t0.674205\t0.0173656\t0.611106\t0.696591\n",
            "5  \t28    \t0.685071\t0.0127742\t0.606028\t0.6943  \n",
            "6  \t38    \t0.687107\t0.0153657\t0.620431\t0.700368\n",
            "7  \t38    \t0.688507\t0.0208491\t0.568029\t0.701131\n",
            "8  \t33    \t0.683834\t0.0458143\t0.474374\t0.702514\n",
            "9  \t28    \t0.696262\t0.00880264\t0.650356\t0.702992\n",
            "10 \t30    \t0.697026\t0.0104196 \t0.632177\t0.702992\n",
            "11 \t26    \t0.698606\t0.00682922\t0.665303\t0.703414\n",
            "12 \t30    \t0.696827\t0.0132032 \t0.627962\t0.703691\n",
            "13 \t33    \t0.700209\t0.00510466\t0.667358\t0.703691\n",
            "14 \t35    \t0.699298\t0.0110421 \t0.63099 \t0.705537\n",
            "15 \t26    \t0.699958\t0.0111172 \t0.6281  \t0.705537\n",
            "16 \t22    \t0.70014 \t0.0133487 \t0.60922 \t0.705537\n",
            "17 \t26    \t0.698172\t0.0176622 \t0.60619 \t0.705537\n",
            "18 \t29    \t0.699202\t0.0156261 \t0.621668\t0.705537\n",
            "19 \t27    \t0.703113\t0.00393487\t0.685224\t0.706046\n",
            "20 \t31    \t0.696372\t0.0350133 \t0.47389 \t0.706046\n",
            "GA Feature Selection finished in 1099.00 seconds.\n",
            "\n",
            "GA selected 81 features out of 152.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 8. Tune and Train Final Models (XGBoost) using Selected Features\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint, uniform # Use uniform for learning_rate etc.\n",
        "\n",
        "print(\"\\n--- Tuning and Training Final XGBoost Models on Selected Features ---\")\n",
        "\n",
        "# Select the best features from the training and test sets (as before)\n",
        "X_train_selected = X_train_proc[:, selected_feature_indices]\n",
        "X_test_selected = X_test_proc[:, selected_feature_indices]\n",
        "\n",
        "print(f\"X_train_selected shape: {X_train_selected.shape}\")\n",
        "print(f\"X_test_selected shape: {X_test_selected.shape}\")\n",
        "\n",
        "# --- 1. Tune and Train Final Classifier (XGBoost) ---\n",
        "\n",
        "# Define parameter distribution for Randomized Search (XGBoost specific)\n",
        "param_dist_clf_xgb = {\n",
        "    'n_estimators': randint(low=100, high=500),\n",
        "    'max_depth': randint(low=3, high=10), # XGBoost often uses shallower trees\n",
        "    'learning_rate': uniform(loc=0.01, scale=0.2), # Sample between 0.01 and 0.21\n",
        "    'subsample': uniform(loc=0.6, scale=0.4), # Sample between 0.6 and 1.0\n",
        "    'colsample_bytree': uniform(loc=0.6, scale=0.4), # Sample between 0.6 and 1.0\n",
        "    'gamma': uniform(loc=0, scale=0.5), # Minimum loss reduction\n",
        "    'scale_pos_weight': [y_train_class.value_counts()[0] / y_train_class.value_counts()[1] if y_train_class.value_counts()[1] > 0 else 1] # Use previously calculated ratio\n",
        "}\n",
        "\n",
        "# Create a base classifier instance\n",
        "xgb_clf_tune = xgb.XGBClassifier(random_state=42,\n",
        "                                 use_label_encoder=False,\n",
        "                                 eval_metric='logloss',\n",
        "                                 n_jobs=1) # Set n_jobs=1 in base estimator for tuning\n",
        "\n",
        "# Setup Randomized Search\n",
        "n_iter_search = 50 # Number of parameter settings to sample\n",
        "random_search_clf_xgb = RandomizedSearchCV(estimator=xgb_clf_tune,\n",
        "                                         param_distributions=param_dist_clf_xgb,\n",
        "                                         n_iter=n_iter_search,\n",
        "                                         cv=3,\n",
        "                                         verbose=1,\n",
        "                                         random_state=42,\n",
        "                                         n_jobs=-1, # Use cores for CV folds\n",
        "                                         scoring='accuracy') # Or 'f1_weighted'\n",
        "\n",
        "print(f\"\\nTuning final XGBoost classifier ({n_iter_search} iterations)...\")\n",
        "start_time_clf_tune = time.time()\n",
        "random_search_clf_xgb.fit(X_train_selected, y_train_class)\n",
        "end_time_clf_tune = time.time()\n",
        "print(f\"Classifier tuning finished in {end_time_clf_tune - start_time_clf_tune:.2f} seconds.\")\n",
        "\n",
        "print(\"\\nBest parameters found for classifier:\", random_search_clf_xgb.best_params_)\n",
        "print(f\"Best cross-validation score ({random_search_clf_xgb.scoring}): {random_search_clf_xgb.best_score_:.4f}\")\n",
        "\n",
        "# Use the best estimator found by the search as the final classifier\n",
        "final_classifier = random_search_clf_xgb.best_estimator_\n",
        "print(\"Final classifier set to the best XGBoost estimator found.\")\n",
        "\n",
        "\n",
        "# --- 2. Tune and Train Final Regressor (XGBoost) ---\n",
        "\n",
        "# Define parameter distribution for Randomized Search (XGBoost specific)\n",
        "param_dist_reg_xgb = {\n",
        "    'n_estimators': randint(low=100, high=500),\n",
        "    'max_depth': randint(low=3, high=10),\n",
        "    'learning_rate': uniform(loc=0.01, scale=0.2),\n",
        "    'subsample': uniform(loc=0.6, scale=0.4),\n",
        "    'colsample_bytree': uniform(loc=0.6, scale=0.4),\n",
        "    'gamma': uniform(loc=0, scale=0.5),\n",
        "    'reg_alpha': uniform(loc=0, scale=1), # L1 regularization\n",
        "    'reg_lambda': uniform(loc=0, scale=1) # L2 regularization\n",
        "}\n",
        "\n",
        "# Create a base regressor instance\n",
        "xgb_reg_tune = xgb.XGBRegressor(random_state=42,\n",
        "                                eval_metric='rmse',\n",
        "                                n_jobs=1) # Set n_jobs=1 in base estimator\n",
        "\n",
        "# Setup Randomized Search\n",
        "random_search_reg_xgb = RandomizedSearchCV(estimator=xgb_reg_tune,\n",
        "                                         param_distributions=param_dist_reg_xgb,\n",
        "                                         n_iter=n_iter_search,\n",
        "                                         cv=3,\n",
        "                                         verbose=1,\n",
        "                                         random_state=42,\n",
        "                                         n_jobs=-1, # Use cores for CV folds\n",
        "                                         scoring='r2') # Or 'neg_root_mean_squared_error'\n",
        "\n",
        "print(f\"\\nTuning final XGBoost regressor ({n_iter_search} iterations)...\")\n",
        "start_time_reg_tune = time.time()\n",
        "random_search_reg_xgb.fit(X_train_selected, y_train_reg)\n",
        "end_time_reg_tune = time.time()\n",
        "print(f\"Regressor tuning finished in {end_time_reg_tune - start_time_reg_tune:.2f} seconds.\")\n",
        "\n",
        "print(\"\\nBest parameters found for regressor:\", random_search_reg_xgb.best_params_)\n",
        "print(f\"Best cross-validation score ({random_search_reg_xgb.scoring}): {random_search_reg_xgb.best_score_:.4f}\")\n",
        "\n",
        "# Use the best estimator found by the search as the final regressor\n",
        "final_regressor = random_search_reg_xgb.best_estimator_\n",
        "print(\"Final regressor set to the best XGBoost estimator found.\")\n",
        "\n",
        "print(\"\\n--- Final XGBoost Model Training/Tuning Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhWdBeLCtfy4",
        "outputId": "2a6d4441-91ec-49dc-903e-89406937a12f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Tuning and Training Final XGBoost Models on Selected Features ---\n",
            "X_train_selected shape: (6000, 81)\n",
            "X_test_selected shape: (2000, 81)\n",
            "\n",
            "Tuning final XGBoost classifier (50 iterations)...\n",
            "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:40:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier tuning finished in 138.89 seconds.\n",
            "\n",
            "Best parameters found for classifier: {'colsample_bytree': np.float64(0.8447411578889518), 'gamma': np.float64(0.06974693032602092), 'learning_rate': np.float64(0.06842892970704363), 'max_depth': 9, 'n_estimators': 289, 'scale_pos_weight': np.float64(0.30406433384046944), 'subsample': np.float64(0.6362425738131283)}\n",
            "Best cross-validation score (accuracy): 0.8853\n",
            "Final classifier set to the best XGBoost estimator found.\n",
            "\n",
            "Tuning final XGBoost regressor (50 iterations)...\n",
            "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
            "Regressor tuning finished in 249.84 seconds.\n",
            "\n",
            "Best parameters found for regressor: {'colsample_bytree': np.float64(0.6391336642604005), 'gamma': np.float64(0.24580793755841618), 'learning_rate': np.float64(0.10469435415611314), 'max_depth': 3, 'n_estimators': 353, 'reg_alpha': np.float64(0.43385164923797304), 'reg_lambda': np.float64(0.39850473439737344), 'subsample': np.float64(0.8463400392208866)}\n",
            "Best cross-validation score (r2): 0.8300\n",
            "Final regressor set to the best XGBoost estimator found.\n",
            "\n",
            "--- Final XGBoost Model Training/Tuning Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 8. Train Final Models using Selected Features\n",
        "\n",
        "print(\"\\n--- Training Final Models on Selected Features ---\")\n",
        "\n",
        "# Select the best features from the training and test sets\n",
        "try:\n",
        "    X_train_selected = X_train_proc[:, selected_feature_indices]\n",
        "    X_test_selected = X_test_proc[:, selected_feature_indices]\n",
        "    print(f\"X_train_selected shape: {X_train_selected.shape}\")\n",
        "    print(f\"X_test_selected shape: {X_test_selected.shape}\")\n",
        "except IndexError as e_idx:\n",
        "     print(f\"Error selecting features with indices: {e_idx}\")\n",
        "     print(f\"Selected indices: {selected_feature_indices}\")\n",
        "     print(f\"X_train_proc shape: {X_train_proc.shape}\")\n",
        "     raise SystemExit(\"Feature selection failed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJB-9FMMtiLz",
        "outputId": "ffed2f17-843e-464e-d076-9a7825db8061"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Final Models on Selected Features ---\n",
            "X_train_selected shape: (6000, 81)\n",
            "X_test_selected shape: (2000, 81)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Train Final Classifier ---\n",
        "# Use more robust parameters for the final models\n",
        "final_classifier = RandomForestClassifier(n_estimators=150, # Increased estimators\n",
        "                                         random_state=42,\n",
        "                                         n_jobs=-1,\n",
        "                                         max_depth=18,     # Slightly deeper\n",
        "                                         min_samples_split=8, # Adjusted\n",
        "                                         min_samples_leaf=4,  # Added min_samples_leaf\n",
        "                                         class_weight='balanced' # Add class weighting if data is imbalanced\n",
        "                                         )\n",
        "print(\"Training final classifier...\")\n",
        "final_classifier.fit(X_train_selected, y_train_class)\n",
        "print(\"Classifier training complete.\")\n",
        "\n",
        "# --- Train Final Regressor ---\n",
        "final_regressor = RandomForestRegressor(n_estimators=150, # Increased estimators\n",
        "                                        random_state=42,\n",
        "                                        n_jobs=-1,\n",
        "                                        max_depth=18,    # Slightly deeper\n",
        "                                        min_samples_split=8, # Adjusted\n",
        "                                        min_samples_leaf=4   # Added min_samples_leaf\n",
        "                                        )\n",
        "print(\"Training final regressor...\")\n",
        "final_regressor.fit(X_train_selected, y_train_reg)\n",
        "print(\"Regressor training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgmni1P6tlvG",
        "outputId": "6655e934-88f3-4cab-f9e0-82bcf05ce3c1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final classifier...\n",
            "Classifier training complete.\n",
            "Training final regressor...\n",
            "Regressor training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n"
      ],
      "metadata": {
        "id": "rTzLvfrv_kud"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 9. Evaluate Final Models\n",
        "\n",
        "print(\"\\n--- Evaluating Final CLASSIFIER on Test Set (Selected Features) ---\")\n",
        "try:\n",
        "    y_pred_class = final_classifier.predict(X_test_selected)\n",
        "    accuracy = accuracy_score(y_test_class, y_pred_class)\n",
        "    f1 = f1_score(y_test_class, y_pred_class, average='weighted')\n",
        "    print(f\"Classifier Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Classifier Weighted F1-Score: {f1:.4f}\")\n",
        "    # from sklearn.metrics import classification_report, confusion_matrix\n",
        "    # print(classification_report(y_test_class, y_pred_class))\n",
        "    # print(confusion_matrix(y_test_class, y_pred_class))\n",
        "except Exception as e_eval_clf:\n",
        "    print(f\"Error during classifier evaluation: {e_eval_clf}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Evaluating Final REGRESSOR on Test Set (Selected Features) ---\")\n",
        "try:\n",
        "    y_pred_reg = final_regressor.predict(X_test_selected)\n",
        "    r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))\n",
        "    mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
        "    print(f\"Regressor R-squared (R²): {r2:.4f}\")\n",
        "    print(f\"Regressor Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "    print(f\"Regressor Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "except Exception as e_eval_reg:\n",
        "     print(f\"Error during regressor evaluation: {e_eval_reg}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YggbLr5tnZC",
        "outputId": "f318cd48-82ad-4087-c7b7-c274a0a22a56"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating Final CLASSIFIER on Test Set (Selected Features) ---\n",
            "Classifier Accuracy: 0.8710\n",
            "Classifier Weighted F1-Score: 0.8707\n",
            "\n",
            "--- Evaluating Final REGRESSOR on Test Set (Selected Features) ---\n",
            "Regressor R-squared (R²): 0.7594\n",
            "Regressor Root Mean Squared Error (RMSE): 3.5785\n",
            "Regressor Mean Absolute Error (MAE): 2.8056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 10. Save Components & Define Prediction Function\n",
        "\n",
        "# --- Define Min/Max for Percentage Conversion ---\n",
        "# Verify these based on your ACTUAL calculate_ccs function logic\n",
        "THEORETICAL_MIN_CCS = -50 # Example value\n",
        "THEORETICAL_MAX_CCS = 85  # Example value\n",
        "\n",
        "def convert_ccs_to_percentage(ccs_score, min_ccs=THEORETICAL_MIN_CCS, max_ccs=THEORETICAL_MAX_CCS):\n",
        "    \"\"\"Converts a raw CCS score to a percentage (0-100).\"\"\"\n",
        "    if max_ccs == min_ccs: return 50.0\n",
        "    clipped_score = np.clip(ccs_score, min_ccs, max_ccs)\n",
        "    percentage = ((clipped_score - min_ccs) / (max_ccs - min_ccs)) * 100\n",
        "    return percentage\n"
      ],
      "metadata": {
        "id": "cxiQO7YKtpEU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Save Model Components ---\n",
        "# Saving preprocessor, selected indices, classifier, regressor, scale factors, threshold\n",
        "save_components = {\n",
        "    'preprocessor': preprocessor,\n",
        "    'selected_feature_indices': selected_feature_indices,\n",
        "    'classifier': final_classifier,\n",
        "    'regressor': final_regressor,\n",
        "    'min_ccs': THEORETICAL_MIN_CCS,\n",
        "    'max_ccs': THEORETICAL_MAX_CCS,\n",
        "    'compatibility_threshold': compatibility_threshold\n",
        "}\n",
        "\n",
        "# Ensure directory exists (defined earlier) before saving\n",
        "try:\n",
        "    with open(model_save_path, 'wb') as f:\n",
        "        pickle.dump(save_components, f)\n",
        "    print(f\"\\nModel components saved successfully to Google Drive: {model_save_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError saving model components: {e}\")\n",
        "\n",
        "\n",
        "# --- Load Model Function Definition ---\n",
        "def load_combined_model(filepath):\n",
        "    \"\"\"Loads the saved model components.\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"ERROR: Model file not found at {filepath}\")\n",
        "        return None\n",
        "    try:\n",
        "        with open(filepath, 'rb') as f:\n",
        "            components = pickle.load(f)\n",
        "        print(f\"Model components loaded successfully from {filepath}\")\n",
        "        return components\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model components: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- Prediction Function Definition ---\n",
        "def predict_cattle_compatibility(new_data_df, model_components):\n",
        "    \"\"\"\n",
        "    Predicts Yes/No compatibility and percentage score for new cattle data.\n",
        "    \"\"\"\n",
        "    if model_components is None:\n",
        "        print(\"ERROR: Model components not loaded.\"); return None\n",
        "\n",
        "    try:\n",
        "        # Extract components\n",
        "        preprocessor = model_components['preprocessor']\n",
        "        selected_indices = model_components['selected_feature_indices']\n",
        "        classifier = model_components['classifier']\n",
        "        regressor = model_components['regressor']\n",
        "        min_ccs = model_components['min_ccs']\n",
        "        max_ccs = model_components['max_ccs']\n",
        "\n",
        "        # 1. Preprocess the new data\n",
        "        # Ensure input df has columns expected by preprocessor\n",
        "        # Handle potential errors during transform\n",
        "        try:\n",
        "             X_new_processed = preprocessor.transform(new_data_df)\n",
        "        except ValueError as ve:\n",
        "             print(f\"ValueError during preprocessing transform: {ve}\")\n",
        "             print(\"Ensure input DataFrame columns exactly match those used during preprocessor fitting.\")\n",
        "             # Optionally try to get expected columns:\n",
        "             # if hasattr(preprocessor, 'feature_names_in_'):\n",
        "             #      print(\"Preprocessor expected columns:\", preprocessor.feature_names_in_)\n",
        "             # else:\n",
        "             #      # Need to infer expected columns from transformers if possible\n",
        "             #      pass\n",
        "             return None\n",
        "        except Exception as e_prep:\n",
        "            print(f\"Error during preprocessing transform: {e_prep}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "\n",
        "        # 2. Select the features identified by the GA\n",
        "        try:\n",
        "             X_new_selected = X_new_processed[:, selected_indices]\n",
        "        except IndexError as e_idx_pred:\n",
        "             print(f\"Error selecting features during prediction: {e_idx_pred}\")\n",
        "             print(f\"Processed data shape: {X_new_processed.shape}, Selected indices count: {len(selected_indices)}\")\n",
        "             return None\n",
        "\n",
        "\n",
        "        # 3. Predict Class (0/1)\n",
        "        class_predictions = classifier.predict(X_new_selected)\n",
        "\n",
        "        # 4. Predict Raw CCS Score\n",
        "        ccs_predictions = regressor.predict(X_new_selected)\n",
        "\n",
        "        # 5. Convert CCS to Percentage\n",
        "        percentage_predictions = [convert_ccs_to_percentage(score, min_ccs, max_ccs) for score in ccs_predictions]\n",
        "\n",
        "        # 6. Format Output\n",
        "        results = []\n",
        "        for i in range(len(class_predictions)):\n",
        "            prediction_label = \"Yes\" if class_predictions[i] == 1 else \"No\"\n",
        "            results.append({\n",
        "                \"Prediction\": prediction_label,\n",
        "                \"Confidence_Score_Percent\": round(percentage_predictions[i], 2),\n",
        "                \"Raw_CCS_Score\": round(ccs_predictions[i], 2)\n",
        "            })\n",
        "        return results\n",
        "\n",
        "    except Exception as e_pred:\n",
        "        print(f\"An error occurred during prediction steps: {e_pred}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "print(\"\\nHelper functions for loading and prediction defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b1WPbJKtvi4",
        "outputId": "b12e8810-afbc-47b5-8bed-a581d95af5f7"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model components saved successfully to Google Drive: /content/drive/MyDrive/MyModels/cattle_predictor_v5.pkl\n",
            "\n",
            "Helper functions for loading and prediction defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 9. Evaluate Final Models\n",
        "\n",
        "# --- This section calculates and prints the Classifier's Accuracy ---\n",
        "print(\"\\n--- Evaluating Final CLASSIFIER on Test Set (Selected Features) ---\")\n",
        "\n",
        "# 1. Predict class labels (0 or 1) on the test set using the selected features\n",
        "y_pred_class = final_classifier.predict(X_test_selected)\n",
        "\n",
        "# 2. Calculate accuracy by comparing predictions (y_pred_class) to the true labels (y_test_class)\n",
        "accuracy = accuracy_score(y_test_class, y_pred_class)\n",
        "\n",
        "# 3. Calculate F1-score (another useful classification metric)\n",
        "f1 = f1_score(y_test_class, y_pred_class, average='weighted')\n",
        "\n",
        "# 4. Print the results\n",
        "print(f\"Classifier Accuracy: {accuracy:.4f}\") # <-- THIS IS THE ACCURACY ON THE TEST SET\n",
        "print(f\"Classifier Weighted F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Optional detailed report (currently commented out)\n",
        "# from sklearn.metrics import classification_report, confusion_matrix\n",
        "# print(classification_report(y_test_class, y_pred_class))\n",
        "# print(confusion_matrix(y_test_class, y_pred_class))\n",
        "\n",
        "\n",
        "# --- This section evaluates the Regressor (predicting the score) ---\n",
        "print(\"\\n--- Evaluating Final REGRESSOR on Test Set (Selected Features) ---\")\n",
        "# (Code for R², RMSE, MAE follows...)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgQ9tsrGtzqY",
        "outputId": "2e60c255-df6b-48e7-cf51-b8a08eaa2440"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating Final CLASSIFIER on Test Set (Selected Features) ---\n",
            "Classifier Accuracy: 0.8710\n",
            "Classifier Weighted F1-Score: 0.8707\n",
            "\n",
            "--- Evaluating Final REGRESSOR on Test Set (Selected Features) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 11. Example Usage (with Added Sample Pair)\n",
        "\n",
        "import pandas as pd # Ensure pandas is imported in this scope if needed\n",
        "import numpy as np  # Ensure numpy is imported\n",
        "\n",
        "print(\"\\n--- Loading saved model and predicting on dummy data ---\")\n",
        "loaded_model = load_combined_model(model_save_path) # Assumes model_save_path is defined\n",
        "\n",
        "if loaded_model:\n",
        "    # --- Define the specific sample pair ---\n",
        "    sample_pair = {\n",
        "        'Cow': { 'Breed': 'Gir', 'Age': 6, 'Weight': 450, 'Height': 140, 'Milk_Yield': 8,\n",
        "                 'Health_Status': 0, 'Drought_Resistance': 70, 'Temperament': 'Calm',\n",
        "                 # Add other Cow keys expected by X, even if None/NaN in this sample\n",
        "                 'Genetic_Diversity_Score': np.nan, 'Fertility_Rate': np.nan, 'Breeding_Success_Rate': np.nan,\n",
        "                 'Disease_Resistance_Score': np.nan, 'Market_Value': np.nan, 'Mother_Milk_Yield': np.nan,\n",
        "                 'Disease': np.nan, 'Past_Breeding_Success': np.nan, 'Same_Parents': np.nan\n",
        "               },\n",
        "        'Bull': {'Breed': 'Jersey', 'Age': 7, 'Weight': 470, 'Height': 142, 'Health_Status': 0,\n",
        "                 'Mother_Milk_Yield': 9, 'Drought_Resistance': 75, 'Temperament': 'Calm',\n",
        "                 # Add other Bull keys expected by X, even if None/NaN in this sample\n",
        "                 'Milk_Yield': np.nan, 'Genetic_Diversity_Score': np.nan, 'Fertility_Rate': np.nan,\n",
        "                 'Breeding_Success_Rate': np.nan, 'Disease_Resistance_Score': np.nan, 'Market_Value': np.nan,\n",
        "                 'Disease': np.nan, 'Past_Breeding_Success': np.nan, 'Same_Parents': np.nan\n",
        "                },\n",
        "        # Top-level keys expected by X\n",
        "        'Same_Parents': 0,\n",
        "        'Trait_Difference': 18,\n",
        "        'Genetic_Diversity': 8, # This might override individual scores depending on how X was defined\n",
        "        'Fertility_Rate': 65,\n",
        "        'Breeding_Success_Rate': 55,\n",
        "        'Disease_Resistance_Score': 6.5,\n",
        "        'Market_Value': 25000,\n",
        "        'Past_Breeding_Success': 'High'\n",
        "    }\n",
        "\n",
        "    # --- Flatten the sample_pair into a dictionary matching DataFrame columns ---\n",
        "    flat_sample = {}\n",
        "    for prefix, inner_dict in sample_pair.items():\n",
        "        if isinstance(inner_dict, dict):\n",
        "            for key, value in inner_dict.items():\n",
        "                flat_sample[f\"{prefix}_{key}\"] = value\n",
        "        else:\n",
        "            # Handle top-level keys directly\n",
        "            flat_sample[prefix] = inner_dict\n",
        "\n",
        "    # Convert the flattened sample to a DataFrame row\n",
        "    new_pair_df = pd.DataFrame([flat_sample])\n",
        "    print(\"Flattened sample pair prepared.\")\n",
        "\n",
        "\n",
        "    # --- Create original dummy data ---\n",
        "    dummy_data = {\n",
        "        'Cow_Breed': ['Angus', 'Holstein', 'UnknownBreed'], 'Cow_Age': [5, 6, 7], 'Cow_Weight': [550.0, 600.0, 580.0],\n",
        "        'Cow_Height': [130.0, 140.0, 135.0], 'Cow_Milk_Yield': [8.5, 9.0, np.nan], 'Cow_Health_Status': [0, 1, 0],\n",
        "        'Cow_Genetic_Diversity_Score': [7.5, 8.1, 7.0], 'Cow_Fertility_Rate': [60.0, 65.0, 55.0],\n",
        "        'Cow_Breeding_Success_Rate': [50.0, 55.0, 45.0], 'Cow_Drought_Resistance': [70.0, 60.0, 65.0],\n",
        "        'Cow_Disease_Resistance_Score': [6.0, 7.0, 5.5], 'Cow_Market_Value': [15000, 18000, 16000],\n",
        "        'Cow_Temperament': ['Calm', 'Calm', 'Aggressive'], 'Cow_Mother_Milk_Yield': [7.0, 7.5, 6.5],\n",
        "        'Cow_Disease': ['FootRot', 'Mastitis', np.nan], 'Cow_Past_Breeding_Success': ['Moderate', 'High', 'Low'],\n",
        "        'Cow_Same_Parents': [0, 0, 1], # Note: This might conflict with top-level Same_Parents if kept\n",
        "        'Bull_Breed': ['Brahman', 'Angus', 'Brahman'], 'Bull_Age': [4, 5, 6], 'Bull_Weight': [650.0, 680.0, 700.0],\n",
        "        'Bull_Height': [145.0, 150.0, 155.0], 'Bull_Milk_Yield': [np.nan, np.nan, np.nan],\n",
        "        'Bull_Health_Status': [0, 0, 1], 'Bull_Genetic_Diversity_Score': [8.0, 7.8, 7.5],\n",
        "        'Bull_Fertility_Rate': [70.0, 75.0, 68.0], 'Bull_Breeding_Success_Rate': [60.0, 65.0, 58.0],\n",
        "        'Bull_Drought_Resistance': [80.0, 50.0, 75.0], 'Bull_Disease_Resistance_Score': [7.5, 6.5, 7.0],\n",
        "        'Bull_Market_Value': [20000, 22000, 21000], 'Bull_Temperament': ['Aggressive', 'Calm', 'Calm'],\n",
        "        'Bull_Mother_Milk_Yield': [np.nan, np.nan, np.nan], 'Bull_Disease': ['None', 'BLV', 'FootRot'],\n",
        "        'Bull_Past_Breeding_Success': ['High', 'Moderate', 'Moderate'],\n",
        "        'Bull_Same_Parents': [0, 1, 0], # Note: This might conflict with top-level Same_Parents if kept\n",
        "        # --- Top-level combined features (as used in Feature Engineering) ---\n",
        "        'Same_Parents': [0, 1, 0], # Example values matching number of rows\n",
        "        'Trait_Difference': [15, 25, 10],\n",
        "        'Genetic_Diversity': [8, 6, 7.5],\n",
        "        'Fertility_Rate': [65, 70, 60], # Example values\n",
        "        'Breeding_Success_Rate': [55, 60, 50], # Example values\n",
        "        'Disease_Resistance_Score': [6.5, 7.0, 6.0], # Example values\n",
        "        'Market_Value': [25000, 19000, 23000], # Example values\n",
        "        # Add *ALL* other columns present in X (before preprocessing)\n",
        "    }\n",
        "    original_dummy_df = pd.DataFrame(dummy_data)\n",
        "\n",
        "    # --- Concatenate the original dummy data and the new sample pair ---\n",
        "    combined_dummy_df = pd.concat([original_dummy_df, new_pair_df], ignore_index=True)\n",
        "    print(f\"Combined dummy data shape: {combined_dummy_df.shape}\")\n",
        "\n",
        "\n",
        "    # --- Dynamically ensure all columns from training X are present ---\n",
        "    final_dummy_df = None # Initialize\n",
        "    if 'preprocessor' in loaded_model:\n",
        "        try:\n",
        "            # Get expected feature names from the fitted preprocessor\n",
        "            if hasattr(loaded_model['preprocessor'], 'feature_names_in_'):\n",
        "                expected_cols = loaded_model['preprocessor'].feature_names_in_\n",
        "            else:\n",
        "                 print(\"Warning: Cannot automatically determine expected columns from preprocessor. Using columns from Cell 4's X.\")\n",
        "                 # This assumes 'X' from cell 4 is still available and correct. It's less robust.\n",
        "                 if 'X' in globals():\n",
        "                     expected_cols = list(X.columns)\n",
        "                 else:\n",
        "                     raise ValueError(\"Original X dataframe not available to determine expected columns.\")\n",
        "\n",
        "            print(f\"\\nPreprocessor expects {len(expected_cols)} columns for prediction.\")\n",
        "\n",
        "            # Check for missing columns in the combined dummy data\n",
        "            current_dummy_cols = combined_dummy_df.columns\n",
        "            missing_in_dummy = [col for col in expected_cols if col not in current_dummy_cols]\n",
        "            if missing_in_dummy:\n",
        "                print(f\"Adding missing expected columns to combined dummy data: {missing_in_dummy}\")\n",
        "                for col in missing_in_dummy:\n",
        "                    combined_dummy_df[col] = np.nan # Add missing columns with NaN\n",
        "\n",
        "            # Select and reorder columns to match preprocessor's expectation\n",
        "            try:\n",
        "                final_dummy_df = combined_dummy_df[expected_cols] # Ensure correct order and columns\n",
        "                print(\"Combined dummy data columns aligned with preprocessor expectations.\")\n",
        "            except KeyError as e_key:\n",
        "                 print(f\"KeyError aligning dummy data columns: {e_key}. Check column names in dummy data and expected columns.\")\n",
        "                 final_dummy_df = None # Prevent prediction if alignment fails\n",
        "            except Exception as e_align:\n",
        "                 print(f\"Error aligning dummy data columns: {e_align}\")\n",
        "                 final_dummy_df = None\n",
        "\n",
        "        except Exception as e_cols:\n",
        "             print(f\"Error preparing dummy data columns: {e_cols}\")\n",
        "             final_dummy_df = None\n",
        "    else:\n",
        "         print(\"ERROR: Preprocessor not found in loaded model components.\")\n",
        "         final_dummy_df = None\n",
        "\n",
        "    # --- Run Prediction if dummy data is ready ---\n",
        "    if final_dummy_df is not None:\n",
        "        predictions = predict_cattle_compatibility(final_dummy_df, loaded_model) # Use the final aligned df\n",
        "\n",
        "        if predictions:\n",
        "            print(\"\\n--- Predictions for Dummy Data (including added sample) ---\")\n",
        "            results_df = pd.DataFrame(predictions)\n",
        "            # Add an identifier column for clarity\n",
        "            results_df['Source'] = ['Dummy'] * len(original_dummy_df) + ['Added Sample'] * len(new_pair_df)\n",
        "            print(results_df.to_string()) # Print full DataFrame results\n",
        "        else:\n",
        "            print(\"Prediction function returned None (failed).\")\n",
        "    else:\n",
        "         print(\"Dummy data preparation failed, skipping prediction.\")\n",
        "\n",
        "else:\n",
        "    print(\"Could not load model components to run prediction example.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29HSJaZDt2kE",
        "outputId": "98910070-1b0a-42ef-c44c-917e3a26d497"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Loading saved model and predicting on dummy data ---\n",
            "Model components loaded successfully from /content/drive/MyDrive/MyModels/cattle_predictor_v5.pkl\n",
            "Flattened sample pair prepared.\n",
            "Combined dummy data shape: (4, 42)\n",
            "\n",
            "Preprocessor expects 41 columns for prediction.\n",
            "Adding missing expected columns to combined dummy data: ['FE_Age_Diff', 'FE_Weight_Diff_Pct', 'FE_Height_Diff_Pct', 'FE_Milk_Sum', 'FE_Drought_Diff', 'FE_Combined_Health', 'FE_Temperament_Combo']\n",
            "Combined dummy data columns aligned with preprocessor expectations.\n",
            "\n",
            "--- Predictions for Dummy Data (including added sample) ---\n",
            "  Prediction  Confidence_Score_Percent  Raw_CCS_Score        Source\n",
            "0        Yes                     58.49          28.96         Dummy\n",
            "1        Yes                     58.80          29.38         Dummy\n",
            "2        Yes                     53.35          22.02         Dummy\n",
            "3        Yes                     55.26          24.60  Added Sample\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Bull_Milk_Yield']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}